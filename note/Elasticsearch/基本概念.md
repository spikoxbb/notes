[TOC]

# 基本概念

## 文档

- ES是面向文档的，文档是所有可搜索数据的最小单位。

  > 如日志文件的日志项

- 文档会被序列化成JSON格式保存在ES中

  > JSON是由字段组成，每个字段都有对应的类型。
  >
  > 字符串，数值，布尔，日期，二进制，范围类型

- 每个文档都有Unique ID

  > 可自己指定或ES自动生成

### 文档元数据

用于标注文档的相关信息

```java
{
  "_index":"xbb",//文档索引名
  "_type":"_doc",//文档类型名
  "_id":"1",//文档唯一ID
  "_score":14.56789,//相关性打分，本次查询的具体算分
  "_source":{
    ...  //文档原始JSON数据
  }，
  "_all":{
    ...  //整合所有的数据在此，已被废弃
  }
  "_version":"1,//文档版本信息，并发读写时可解决文档冲突问题
}
```

## 索引

Index - 索引是文档的容器，是一类文档的结合。

- Mapping定义文档字段的类型
- Setting定义不同的数据分布，指定用多少的分片，数据是怎样分布的。

### 抽象与类比

| DB     | ES       |
| ------ | -------- |
| TABLE  | Index    |
| Row    | Document |
| Column | Field    |
| Schema | Mapping  |
| SQL    | DSL3     |

## 节点

- 每个节点启动后默认是一个mater eligible节点

  >  可以设置node.master:false禁止

- 当一个节点启动时它会把自己选举成为master节点

- 每个节点都保存了集群的状态信息，只要Master节点才能修改集群的状态信息

  >- 所有的节点信息
  >- 所有的索引和其相关的Mapping和Setting信息
  >- 分片的路由信息

### 主要节点类型

- Data Node

  可以保存数据的节点，负责保存分片数据

- Coordinating Node

  负责接收客户端的请求并分发给其他节点，最终将结果汇总在一起。每个节点都默认起到了Coordinating Node的指责

## 分片

- 主分片用来解决数据水平扩展的问题，通过主分片可以将数据分布在集群内所有节点上。
  1. 一个分片是一个运行的Lucene实例
  2. 主分片数在索引创建时指定，后续不允许修改，除非reindex
- 副本，用于解决高可用问题，是主分片的拷贝
  1. 副本分片数可以动态调整

![WechatIMG7](../img/WechatIMG7.jpeg)

### 查看集群健康状态

GET _cluster/health

- GREEN 主分片与副本都正常分配
- YELLOW 主分片正常分配，有副本分配未能正常分配
- RED 有主分片未能正常分配

## CRUD 

```java
//通过index方式创建，自动生成ID
POST users/_doc
{
  "user":"Mike",
  "post_date" : "2020-08-06T12:17:00",
  "message" : "just practise"
}
//通过create方式创建，指定Id,如果id已经存在，报错
PUT users/_doc/1?op_type=create
{
    "user" : "Jack",
    "post_date" : "2019-05-15T14:12:12",
    "message" : "trying out Elasticsearch"
}
//同上，通过create方式创建，指定Id,如果id已经存在，报错
PUT users/_create/1
{
    "user" : "Jack",
    "post_date" : "2019-05-15T14:12:12",
    "message" : "trying out Elasticsearch"
}
//通过ID获取文档
GET users/_doc/1
//删除原来的文档，添加新的
PUT users/_doc/1
{
    "user" : "Jack"
}
//在原文档上增加字段
POST users/_update/1
{
  "doc":{
    "post_date" : "2019-05-15T14:12:12",
    "message" : "trying out Elasticsearch"
  }
}
//通过ID删除文档
DELETE users/_doc/1
//批量操作文档  
POST _bulk
{"index":{"_index":"test", "_id":1}}
{"filed1":"value1"}
{"delete":{"_index":"test", "_id":2}}
{"create":{"_index":"test2","_id":3}}
{"field1":"value1"}
{"update":{"_index":"test2","_id":3}}
{"doc":{"field3":"value3"}}
//批量通过ID获取文档  
GET /_mget
{
  "docs":[
    {
      "_index":"test",
    "_id":1
    },
    {
      "_index":"test2",
    "_id":3
    }
    ]
}
//批量通过ID获取test下的文档 
GET /test/_mget
{
  "docs":[
    {
    "_id":1
    },
    {
    "_id":3
    }
    ]
}
//批量通过ID获取文档，并查看知道field的原数据
GET /_mget
{
  "docs":[
    {
       "_index" : "test",
        "_id" : "1",
        "_source" : false
    },
    {
       "_index" : "test2",
        "_id" : "3",
        "_source" : ["field1", "field3"]
    },
    {
      "_index":"test2",
      "_id":"3",
      "_source":{
        "include":["user"],
        "exclude":["user.location"]
      }
    }
  ]
}
//mget 是通过文档ID列表得到文档信息，msearch 是根据查询条件，搜索到相应文档。
POST movies/_msearch
{}
{"query":{"match_all":{}}, "size":10}
{"index" : "movies"}
{"query" : {"match_all" : {}},"size":2}
```

## 倒排索引

- 单词词典

  记录所有文档的单词，记录单词到倒排列表的关系，数量庞大，可通过B+数和哈希拉链发法实现

- 倒排列表(posting list)

  记录了单词对应的文档结合，由倒排索引项（posting）组成：

  - 文档ID
  - 词频 单词出现的次数，用于相关性打分
  - 位置 在文档中分词的位置，用于搜索
  - 偏移 记录单词的开始结束为位置，用于高亮显示

| 文档ID | 文档内容   |
| ------ | ---------- |
| 1      | xbb spiko  |
| 2      | xbb lalala |
| 3      | kkk xbb    |

**xbb**:

| 文档ID | TF   | Position | Offset |
| ------ | ---- | -------- | ------ |
| 1      | 1    | 0        | <0,4>  |
| 2      | 1    | 0        | <0,4>  |
| 3      | 1    | 1        | <4,8>  |

​                                                                      **posting list**

ES的Json文档的每个字段都有自己的倒排索引，可以在mapping指定某些字段不做索引

## Analysis和Analyzer

Analysis即分词过程

### Analyzer（分词器）的组成

- character filter 针对原始文本处理，如去除html
- tokenizer 按照规则切分单词
- token filter 对切分的单词进行加工，小写，删除停用词(stopwords 如in,the等)，增加同义词

### 常用分词器 

- standard analyzer

  > tokenizer 
  >
  > - standard 按词切分 
  >
  > token filter
  >
  > - standard 
  > - lower case 小写处理
  > - stop(默认关闭)

- simple analyzer

  > tokenizer 
  >
  > - lower case 按照非字母切分（非字母的会被去除），小写处理 

- whitespace analyzer

  > tokenizer 
  >
  > - whitespace 按照空格切分

- stop analyzer

  > tokenizer 
  >
  > - lower case 按照非字母切分（非字母的会被去除），小写处理 
  >
  > token filter
  >
  > - stop 相比simple analyzer多了stop filter，会去掉the,a等修饰词

- keyword analyzer

  > tokenizer 
  >
  > - keyword 不分词

- pattern analyzer

  > tokenizer 
  >
  > - Pattern 通过正则表达式进行分词，默认是\W+，非字母的符号进行分词
  >
  > token filter
  >
  > - lowercase 
  > - stop(默认关闭)

- language analyzers 各语言的分词器如 English analyzer

### api

> Get _analyze
> {
>    "analyzer": "icu_analyzer ",
>   "text": "熊爸爸是个大美人"
> }
