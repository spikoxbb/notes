[TOC]

# 自定义分词

### Character Filter

对文本进行处理，例如增加删除替换文本，会影响Tokenizer的position和offset信息

- Html strip

  去除Html标签

- Mapping

  字符串替换

  ```java
  POST _analyze
  {
    "tokenizer": "standard",
    "char_filter": [
        {
          "type" : "mapping",
          "mappings" : [ "- => ."]
        }
      ],
     "text": "123-456, I-test! test-990 650-555-1234"
  }
  ```

- Pattern replace

  正则匹配替换

  ```java
  GET _analyze
  {
    "tokenizer": "standard",
    "char_filter": [
        {
          "type" : "pattern_replace",
          "pattern" : "http://(.*)",
          "replacement" : "$1"
        }
      ],
      "text" : "http://www.elastic.co"
  }
  
  GET _analyze
  {
    "tokenizer": "standard",
    "char_filter": [
        {
          "type" : "pattern_replace",
          "pattern" : "h.*a"
        }
      ],
      "text" : "http://www.elastic.co"
  }
  //res:
  {
    "tokens" : [
      {
        "token" : "stic.co",
        "start_offset" : 14,
        "end_offset" : 21,
        "type" : "<ALPHANUM>",
        "position" : 0
      }
    ]
  }
  ```

### Tokenizer

按规则分词

### Token Filter

将Tokenizer生成的单词进行增删改

- Lowercase

- stop

- synonym

  添加近义词

```java
//"stop","lowercase位置互换才可去除”In“
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["stop","lowercase"],
  "text": ["The gilrs In China are playing this game!"]
}
```
